{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Computer Vision Exercise Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure selected kernel for the notebook is \"venv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Computer Images and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display two test images\n",
    "\n",
    "# open two test images\n",
    "aerial = cv2.imread('../data/rareplanes6.jpg')\n",
    "dog_raw = cv2.imread('../data/dog.jpg')  # default: bgr for display\n",
    "print(f\"arr1 shape (H x W x Channels) or (Rows x Cols x Channels): {dog_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dog_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(aerial)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dog_raw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks a little...blue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv.imread() defaults to bgr\n",
    "# Use openCV to reverse channel order\n",
    "plt.imshow(cv2.cvtColor(dog_raw, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert channels a different way\n",
    "dog = dog_raw[..., ::-1]                  # reverses order of last dim of array bgr -> rgb\n",
    "aerial = aerial[..., ::-1]\n",
    "plt.imshow(dog)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulating the image is achieved by manipulating the array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropping the image is a simple as indexing the array\n",
    "cropped = dog[:100,:100,:]\n",
    "plt.imshow(cropped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own crop\n",
    "# full dims = 576 x 768\n",
    "BOTTOM =\n",
    "TOP = \n",
    "LEFT = \n",
    "RIGHT = \n",
    "cropped2 = dog[BOTTOM:TOP,:LEFT,:RIGHT]\n",
    "plt.imshow(cropped2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit color channels\n",
    "# Channel order: RGB\n",
    "no_green = dog.copy()\n",
    "no_green[:,:,1] = 0\n",
    "\n",
    "plt.imshow(no_green)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimming is a matter of division!\n",
    "dimmed = (dog.copy() / 2).astype(int)\n",
    "\n",
    "plt.imshow(dimmed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first 20 elements of the first row of each image's Green channel\n",
    "print(dimmed[0,:20,1])\n",
    "print(dog[0,:20,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can you think of your own image transformation, and then implement it?\n",
    "custom = dog.copy()\n",
    "\n",
    "# suggestions\n",
    "# set some subset of pixels to 0\n",
    "# add/subtract some constant from all or some pixels\n",
    "custom = # your transformation goes here\n",
    "\n",
    "plt.imshow(custom)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we multiply?\n",
    "doubled = (dog.copy() * 2).astype(int)\n",
    "\n",
    "plt.imshow(doubled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huh? Some are brighter but some are dimmer\n",
    "# inspect the 200-215th elements of the first row of each image's Green channel\n",
    "print(dog[0,:15,1])\n",
    "print(doubled[0,:15,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the 200-215th elements of the first row of each image's Green channel\n",
    "print(dog[0,200:215,1])\n",
    "print(doubled[0,200:215,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = np.array([188, 204, 240, 236, 224, 224, 218, 203, 186, 231])\n",
    "doubled_for_real = original * 2\n",
    "double_modulo = (original * 2) % 256\n",
    "print(original)\n",
    "print(doubled_for_real)\n",
    "print(double_modulo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RGB can't represent values over 255, so instead we're seeing (arr1 * 2) modulo 256   \n",
    "How does the computer know to do this atuomatically??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dog.dtype)\n",
    "print(original.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is the data type! The datatype of `arr2` is 'uint8' which stands for \"unsigned 8-bit integer\". When we initially read in this image to the variable `arr1` with the code `arr1 = cv2.imread('../data/dog.jpg')`, it was automatically encoded as 'uint8'.  This data type represents integers with 8 binary digits (bits). It ranges from 00000000 to 11111111 (which in decimal is 255). In other words this data type is only expressive enough to represent intergers in the range [0,255]. Furthermore if an operation results in a value greater than 255, than uint8 will \"wrap around\" using modulo arithmetic as we observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer vision models require their inputs to be in very specific formats. Often it is neccesary to modify an image to meet these input specifications. For example, the model we'll be using today (tiny-yolov3) requires it's input image array values all to be scaled to the range [0,1] and have the dimensions (1x3x416x416)\n",
    "\n",
    "https://github.com/onnx/models/tree/main/validated/vision/object_detection_segmentation/tiny-yolov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letterbox procedure\n",
    "def letterbox(src, dest_shape):\n",
    "    ## INPUTS ##\n",
    "        #  '''resize image with unchanged aspect ratio using padding'''\n",
    "        # src - an image array\n",
    "        # dest_shape - tuple specifying desired letterboxed image dimensions\n",
    "    ## Output ##\n",
    "        # dest - letterboxed image array\n",
    "    \n",
    "    # get src dims\n",
    "    src_width = src.shape[1]    # img.shape returns tuple (rows, cols, chan)\n",
    "    src_height = src.shape[0]   # NOTE: rows => height; cols => width\n",
    "\n",
    "    # cons dest array (filled with gray), get dest dims\n",
    "    # NOTE: each 32-bit [B, G, R, A] pixel value is [128, 128, 128, 255]\n",
    "    dest = np.full(dest_shape, np.uint8(128))\n",
    "    if dest_shape[2] > 3:\n",
    "        dest[:, :, 3] = np.uint8(255)\n",
    "    dest_width = dest.shape[1]\n",
    "    dest_height = dest.shape[0]\n",
    "\n",
    "    # calculate width and height ratios\n",
    "    width_ratio = dest_width / src_width        # NOTE: ratios are float values\n",
    "    height_ratio = dest_height / src_height\n",
    "\n",
    "    # init resized image width and height with max values (dest dims)\n",
    "    rsz_width = dest_width\n",
    "    rsz_height = dest_height\n",
    "\n",
    "    # smallest scale factor will scale other dimension as well\n",
    "    if width_ratio < height_ratio:\n",
    "        rsz_height = int(src_height * width_ratio)  # NOTE: integer truncation\n",
    "    else:\n",
    "        rsz_width = int(src_width * height_ratio)\n",
    "\n",
    "    # resize the image data using bi-linear interpolation\n",
    "    rsz_dims = (rsz_width, rsz_height)\n",
    "    rsz = cv2.resize(src, rsz_dims, 0, 0, cv2.INTER_LINEAR)\n",
    "\n",
    "    # embed rsz into the center of dest\n",
    "    dx = int((dest_width - rsz_width) / 2)          # NOTE: integer truncation\n",
    "    dy = int((dest_height - rsz_height) / 2)\n",
    "    dest[dy:dy+rsz_height, dx:dx+rsz_width, :] = rsz\n",
    "    rsz_origin = (dx, dy)\n",
    "\n",
    "    # letterboxing complete, return dest\n",
    "    return (dest, rsz_origin, rsz_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letterbox the image to resize for NN input (size: (height, width, chan))\n",
    "letterboxed_dog = letterbox(dog, (416, 416, 3))[0]\n",
    "letterboxed_aerial = letterbox(aerial, (416,416,3))[0]\n",
    "plt.imshow(letterboxed_dog)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack_buffer procedure, ONNX model expects normalized float32 NCHW tensor\n",
    "def pack_buffer(src):\n",
    "    dest = np.array(src, dtype='float32')       # cons dest array via copy\n",
    "    if dest.shape[2] > 3:\n",
    "        dest = dest[:,:,:3]                     # if there is an alpha channel, remove it\n",
    "    #dest = dest [..., ::-1]                     # reorder channels BGR -> RGB\n",
    "    dest /= 255.0                               # normalize vals\n",
    "    dest = np.transpose(dest, [2, 0, 1])        # make channel first dim\n",
    "    dest = np.expand_dims(dest, 0)              # ins batch dim before chan dim\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered_dog = pack_buffer(dog)\n",
    "buffered_dog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the values are no longer integers\n",
    "buffered_dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered_dog.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Float\" or floating-point numbers are used to represent real numbers (as opposed to integers--think: fractions and decimals). \"32\" refers to the number of bits allocated to store each floating-point number. 32 bits = 4 bytes.  \n",
    "Representing floating point numbers with a fixed number of bits entails a tradeoff between the range of values you can represent and their precision.  \n",
    "TODO: briefly talk about how floating point numbers are stored (sign, exponent, significand/mantissa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: PTNNS and Convolutional Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of manual convolution\n",
    "print(\"\\nManual Convolution Demonstration:\")\n",
    "sample_image = np.array([[50, 60, 70],\n",
    "                         [80, 90, 100],\n",
    "                         [110, 120, 130]])\n",
    "\n",
    "sample_kernel = np.array([[0, 1, 0],\n",
    "                          [1, -4, 1],\n",
    "                          [0, 1, 0]])\n",
    "\n",
    "print(\"Sample Image:\")\n",
    "print(sample_image)\n",
    "print(\"\\nSample Kernel:\")\n",
    "print(sample_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform manual convolution for the center pixel\n",
    "result = (60 * 1 + 80 * 1 + 100 * 1 + 90 * -4 + 120 * 1)\n",
    "print(f\"\\nManual calculation for center pixel: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform convolution using scipy\n",
    "convolved = signal.convolve2d(sample_image, sample_kernel, mode='same', boundary='symm')\n",
    "print(\"\\nFull convolution result:\")\n",
    "print(convolved.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of kernels using in image processing\n",
    "1. Identity: doesn't change the image\n",
    "2. Blur: averages nearby pixels\n",
    "3. Sharpen: enhances edges by increasing contrast with neighboring pixels\n",
    "4. Edge Detection: highlights edges in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kernel(image, kernel):\n",
    "    # Apply the kernel to each color channel\n",
    "    r = signal.convolve2d(image[:,:,0], kernel, mode='same', boundary='symm')\n",
    "    g = signal.convolve2d(image[:,:,1], kernel, mode='same', boundary='symm')\n",
    "    b = signal.convolve2d(image[:,:,2], kernel, mode='same', boundary='symm')\n",
    "    \n",
    "    # Stack the channels back together\n",
    "    return np.stack([r, g, b], axis=2).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_kernel_effect(image, kernel, kernel_name):\n",
    "    # Apply the kernel\n",
    "    convolved = apply_kernel(image, kernel)\n",
    "    \n",
    "    # Select a small region to display pixel values (e.g., 5x5)\n",
    "    region_original = image[100:105, 100:105, 0]  # Red channel of original\n",
    "    region_convolved = convolved[100:105, 100:105, 0]  # Red channel of convolved\n",
    "    \n",
    "    # Display images and pixel values\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle(f'Effect of {kernel_name} Kernel', fontsize=16)\n",
    "    \n",
    "    axs[0, 0].imshow(image)\n",
    "    axs[0, 0].set_title('Original Image')\n",
    "    axs[0, 0].axis('off')\n",
    "    \n",
    "    axs[0, 1].imshow(convolved)\n",
    "    axs[0, 1].set_title('Convolved Image')\n",
    "    axs[0, 1].axis('off')\n",
    "    \n",
    "    axs[1, 0].imshow(region_original, cmap='gray', vmin=0, vmax=255)\n",
    "    axs[1, 0].set_title('Original Pixel Values (5x5 region)')\n",
    "    for (j,i),label in np.ndenumerate(region_original):\n",
    "        axs[1, 0].text(i,j,int(label),ha='center',va='center')\n",
    "    \n",
    "    axs[1, 1].imshow(region_convolved, cmap='gray', vmin=0, vmax=255)\n",
    "    axs[1, 1].set_title('Convolved Pixel Values (5x5 region)')\n",
    "    for (j,i),label in np.ndenumerate(region_convolved):\n",
    "        axs[1, 1].text(i,j,int(label),ha='center',va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the kernel\n",
    "    print(f\"\\n{kernel_name} Kernel:\")\n",
    "    print(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image (replace with any image file)\n",
    "#image_path = \"sample_image.jpg\"\n",
    "#image = np.array(Image.open(image_path))\n",
    "image = dog\n",
    "# Define different convolution kernels\n",
    "kernels = {\n",
    "    'Identity': np.array([[0, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 0, 0]]),\n",
    "    \n",
    "    'Average Blur': np.array([[1, 1, 1],\n",
    "                      [1, 1, 1],\n",
    "                      [1, 1, 1]]) / 9,\n",
    "    \n",
    "    'Gaussian Blur': np.array([[1, 4, 6, 4, 1],\n",
    "                                     [4, 16, 24, 16, 4],\n",
    "                                     [6, 24, 36, 24, 6],\n",
    "                                     [4, 16, 24, 16, 4],\n",
    "                                     [1, 4, 6, 4, 1]]) / 256,\n",
    "    \n",
    "    'Sharpen': np.array([[0, -1, 0],\n",
    "                         [-1, 5, -1],\n",
    "                         [0, -1, 0]]),\n",
    "    \n",
    "    'Edge Detection': np.array([[0, -1, 0],\n",
    "                                [-1,  4, -1],\n",
    "                                [0, -1, 0]])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply each kernel and show detailed results\n",
    "for name, kernel in kernels.items():\n",
    "    show_kernel_effect(image, kernel, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edge detection here is pretty bad! There is lots of noise in the photo, which makes detecting trued edges difficult. How might we resolve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_kernel_effect(apply_kernel(dog, kernels['Gaussian Blur']), kernels['Edge Detection'], \"Blur + Edge Detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a kernel to understand its structure\n",
    "print(\"Structure of the Sharpen kernel:\")\n",
    "print(kernels['Sharpen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own kernel\n",
    "\n",
    "# Edit this kernel\n",
    "custom_kernel = np.array([[0, 0, 0],\n",
    "                          [0, 1, 0],\n",
    "                          [0, 0, 0]])\n",
    "\n",
    "kernels['Custom'] = custom_kernel\n",
    "\n",
    "show_kernel_effect(dog, kernels['Custom'], \"Custom Kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Dimension change with Stride and Pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def apply_convolution(input_array, kernel, stride):\n",
    "    input_height, input_width = input_array.shape\n",
    "    kernel_size = kernel.shape[0]\n",
    "    \n",
    "    output_height = (input_height - kernel_size) // stride + 1\n",
    "    output_width = (input_width - kernel_size) // stride + 1\n",
    "    \n",
    "    output = np.zeros((output_height, output_width))\n",
    "    \n",
    "    for i in range(0, input_height - kernel_size + 1, stride):\n",
    "        for j in range(0, input_width - kernel_size + 1, stride):\n",
    "            output[i//stride, j//stride] = np.sum(input_array[i:i+kernel_size, j:j+kernel_size] * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def apply_pooling(input_array, pool_size, stride, mode='max'):\n",
    "    input_height, input_width = input_array.shape\n",
    "    \n",
    "    output_height = (input_height - pool_size) // stride + 1\n",
    "    output_width = (input_width - pool_size) // stride + 1\n",
    "    \n",
    "    output = np.zeros((output_height, output_width))\n",
    "    \n",
    "    for i in range(0, input_height - pool_size + 1, stride):\n",
    "        for j in range(0, input_width - pool_size + 1, stride):\n",
    "            if mode == 'max':\n",
    "                output[i//stride, j//stride] = np.max(input_array[i:i+pool_size, j:j+pool_size])\n",
    "            elif mode == 'average':\n",
    "                output[i//stride, j//stride] = np.mean(input_array[i:i+pool_size, j:j+pool_size])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample input array\n",
    "input_array = np.array([\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 0],\n",
    "    [0, 1, 1, 0, 0]\n",
    "])\n",
    "\n",
    "# Define a simple edge detection kernel\n",
    "# kernel = np.array([[1, 1, 1],\n",
    "#                       [1, 1, 1],\n",
    "#                       [1, 1, 1]]) / 9\n",
    "\n",
    "kernel = np.array([[0,0,0],\n",
    "                   [0,1,0],\n",
    "                   [0,0,0]])\n",
    "# Demonstrate convolution with different strides\n",
    "strides = [1, 2]\n",
    "fig, axs = plt.subplots(1, len(strides) + 1, figsize=(15, 5))\n",
    "fig.suptitle('Convolution with Different Strides', fontsize=16)\n",
    "\n",
    "axs[0].imshow(input_array, cmap='gray')\n",
    "axs[0].set_title('Input Array')\n",
    "axs[0].axis('off')\n",
    "\n",
    "for i, stride in enumerate(strides, 1):\n",
    "    output = apply_convolution(input_array, kernel, stride)\n",
    "    axs[i].imshow(output, cmap='gray')\n",
    "    axs[i].set_title(f'Stride = {stride}\\nOutput Shape: {output.shape}')\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrate pooling\n",
    "pool_size = 2\n",
    "strides = [1, 2]\n",
    "pooling_modes = ['max', 'average']\n",
    "\n",
    "fig, axs = plt.subplots(len(pooling_modes), len(strides) + 1, figsize=(15, 10))\n",
    "fig.suptitle('Pooling with Different Strides and Modes', fontsize=16)\n",
    "\n",
    "for i, mode in enumerate(pooling_modes):\n",
    "    axs[i, 0].imshow(input_array, cmap='gray')\n",
    "    axs[i, 0].set_title('Input Array')\n",
    "    axs[i, 0].axis('off')\n",
    "    \n",
    "    for j, stride in enumerate(strides, 1):\n",
    "        output = apply_pooling(input_array, pool_size, stride, mode)\n",
    "        axs[i, j].imshow(output, cmap='gray')\n",
    "        axs[i, j].set_title(f'{mode.capitalize()} Pooling\\nStride = {stride}\\nOutput Shape: {output.shape}')\n",
    "        axs[i, j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print numerical values for better understanding\n",
    "print(\"Input Array:\")\n",
    "print(input_array)\n",
    "\n",
    "print(\"\\nConvolution Output (Stride = 1):\")\n",
    "print(apply_convolution(input_array, kernel, 1))\n",
    "\n",
    "print(\"\\nConvolution Output (Stride = 2):\")\n",
    "print(apply_convolution(input_array, kernel, 2))\n",
    "\n",
    "print(\"\\nMax Pooling Output (2x2, Stride = 1):\")\n",
    "print(apply_pooling(input_array, 2, 1, 'max'))\n",
    "\n",
    "print(\"\\nMax Pooling Output (2x2, Stride = 2):\")\n",
    "print(apply_pooling(input_array, 2, 2, 'max'))\n",
    "\n",
    "print(\"\\nAverage Pooling Output (2x2, Stride = 2):\")\n",
    "print(apply_pooling(input_array, 2, 2, 'average'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Trained Neural Networks (PTNNs)\n",
    "    a.\tPTNNs have architecture and trained weights.  \n",
    "    b.\tGetting trained Tiny YOLOv3 from ONNX model zoo  \n",
    "    c.\tConsider NETRON model viewer (https://github.com/lutzroeder/netron)  \n",
    "    d.\tonnx2torch module  \n",
    "    e.\tLoading ONNX model into pytorch  \n",
    "    f.\tRun on a test image look at output: bbox center and extent, objectness, classifications.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the model classes\n",
    "def read_model_classes(pathname = '../model/coco.names'):\n",
    "    file = open(pathname, 'r')\n",
    "    classes = []\n",
    "    while True:\n",
    "        class_name = file.readline().strip()\n",
    "        if not class_name:\n",
    "            break\n",
    "        classes.append(class_name)\n",
    "    file.close()\n",
    "    return classes\n",
    "\n",
    "def run_inference(model, image_array):\n",
    "    # cons input for ONNX model inference (packed images and their orig dims)\n",
    "    img = pack_buffer(image_array)\n",
    "    # dim4 = np.array([image_array.shape[1], image_array.shape[0]], dtype=np.float32).reshape(1, 2)\n",
    "\n",
    "    # run ONNX model inference on input buffer to get results\n",
    "    return model.run(None, {'input_1': img })#,'image_shape': dim4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_names = read_model_classes()\n",
    "for item in coco_names:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cons ONNX Tiny YOLOv3 NN model\n",
    "model   = ort.InferenceSession('../model/modified_yolov3-tiny.onnx')\n",
    "# model = ort.InferenceSession('../model/yolov3-tiny.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_results1 = run_inference(model, letterboxed_dog)\n",
    "aerial_results1 = run_inference(model, letterboxed_aerial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our results!\n",
    "dog_results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dog_results1))     \n",
    "print(dog_results1[0].shape)\n",
    "print(dog_results1[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_results1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Bounding Boxes for Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_annos procedure (fixed ONNX anno scaling in unscale_annos proc)\n",
    "\n",
    "# don't need to understand this code in detail but i think its important you see it\n",
    "def draw_annos(src, annos, classes):\n",
    "    dest = np.copy(src)\n",
    "    green = (0, 255, 0)\n",
    "    black = (0, 0, 0)\n",
    "    face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    scale = 0.5\n",
    "    thickness = 1\n",
    "    for anno in annos:\n",
    "        pt1 = (anno[0][0], anno[0][1])\n",
    "        pt2 = (anno[0][2], anno[0][3])\n",
    "        text = f'{classes[anno[2]]}: {anno[1]:.2f}'\n",
    "        (w, h), _ = cv2.getTextSize(text, face, scale, thickness)\n",
    "        pt3 = (pt1[0], pt1[1] - h)\n",
    "        pt4 = (pt1[0] + w, pt1[1])\n",
    "        dest = cv2.rectangle(dest, pt1, pt2, green)\n",
    "        dest = cv2.rectangle(dest, pt3, pt4, green, cv2.FILLED)\n",
    "        dest = cv2.putText(dest, text, pt1, face, scale, black, thickness)\n",
    "    return dest\n",
    "\n",
    "# unscale_annos procedure (fixes ONNX anno scaling)\n",
    "def unscale_annos(annos, dw, dh, w0, h0, w1, h1):\n",
    "    res = []\n",
    "    scale_w = float(w1) / float(w0)\n",
    "    scale_h = float(h1) / float(h0)\n",
    "    for anno in annos:\n",
    "        pt1 = (int(anno[0][1]), int(anno[0][0]))   # ONNX bug! Points are\n",
    "        pt2 = (int(anno[0][3]), int(anno[0][2]))   # transposed.\n",
    "        pt3 = (pt1[0] - dw, pt1[1] - dh)\n",
    "        pt4 = (pt2[0] - dw, pt2[1] - dh)\n",
    "        pt5 = (int(float(pt3[0]) * scale_w), int(float(pt3[1]) * scale_h))\n",
    "        pt6 = (int(float(pt4[0]) * scale_w), int(float(pt4[1]) * scale_h))\n",
    "        arr1 = np.array([pt5[0], pt5[1], pt6[0], pt6[1]], dtype='int32')\n",
    "        res.append((arr1, anno[1], anno[2]))\n",
    "    return res\n",
    "\n",
    "#+BEGIN_EXAMPLE\n",
    "\n",
    "# sigmoid procedure\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + math.exp(-x))\n",
    "\n",
    "# (redefined) proc_results procedure\n",
    "def proc_results(res, classes, pobj_thresh = 0.1, pcls_thresh = 0.5, orig_img_size = 416,\n",
    "                 anchors = np.array([[[81,82], [135,169], [344,319]],\n",
    "                                     [[10,14], [ 23, 27], [ 37, 58]]],\n",
    "                                    dtype='int32')):\n",
    "    dets = []\n",
    "    # candidate detection layout:\n",
    "    # [x, y, w, h, pobj, pcls_0, pcls_1, ..., pcls_i]\n",
    "    # i: [0, num_classes)\n",
    "    num_classes = len(classes)\n",
    "    pcls_offset = 5                                     # offset of class probs\n",
    "    num_params = pcls_offset + num_classes              # numParams per cand det\n",
    "    num_yolo_blocks = anchors.shape[0]\n",
    "    num_anchors = anchors.shape[1]\n",
    "    assert len(res) == num_yolo_blocks\n",
    "    for blk in range(num_yolo_blocks):                  # iter over yolo blocks\n",
    "        height_blk = res[blk].shape[1]\n",
    "        width_blk = res[blk].shape[2]\n",
    "        stride_blk = orig_img_size / width_blk          # ASSUMES square image\n",
    "        shape_blk = (height_blk, width_blk, num_anchors, num_params)\n",
    "        dets_blk = np.reshape(res[blk], shape_blk)\n",
    "        # each yolo block has an \"image\" where each \"pixel\" has a candidate\n",
    "        # detection per anchor box\n",
    "        for hi in range(height_blk):                    # iter over img rows\n",
    "            for wi in range(width_blk):                 # iter over img cols\n",
    "                for ai in range(num_anchors):           # iter over pxl anchors\n",
    "                    det = dets_blk[hi][wi][ai]          # get detection\n",
    "                    pobj = sigmoid(det[4])              # get objectness prob\n",
    "                    if pobj > pobj_thresh:\n",
    "                        x = stride_blk * (wi + sigmoid(det[0]))\n",
    "                        y = stride_blk * (hi + sigmoid(det[1]))\n",
    "                        w = math.exp(det[2]) * anchors[blk][ai][0]\n",
    "                        h = math.exp(det[3]) * anchors[blk][ai][1]\n",
    "                        for ci in range(num_classes):\n",
    "                            pcls = sigmoid(det[pcls_offset + ci])\n",
    "                            if pcls > pcls_thresh:\n",
    "                                x1, y1 = x - (w / 2.0), y - (h / 2.0)\n",
    "                                x2, y2 = x + (w / 2.0), y + (h / 2.0)\n",
    "                                dets.append((pobj, pcls, ci, x1, y1, x2, y2))\n",
    "    return dets\n",
    "\n",
    "# overlap procedure, find bbox overlap length along a dim\n",
    "def overlap(lo1, hi1, lo2, hi2):\n",
    "    lo = max(lo1, lo2)\n",
    "    hi = min(hi1, hi2)\n",
    "    return hi - lo\n",
    "\n",
    "# iou procedure (intersection-over-union); bbox: [xl, yl, xh, yh]\n",
    "def iou(bbox1, bbox2):\n",
    "    area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])   # bbox1 area\n",
    "    area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])   # bbox2 area\n",
    "    wo = overlap(bbox1[0], bbox1[2], bbox2[0], bbox2[2])    # overlap x dim\n",
    "    ho = overlap(bbox1[1], bbox1[3], bbox2[1], bbox2[3])    # overlap y dim\n",
    "    i_area = (wo * ho) if (wo > 0.0 and ho > 0.0) else 0.0  # intersection area\n",
    "    u_area = area1 + area2 - i_area                         # union area\n",
    "    return i_area / u_area\n",
    "\n",
    "# basic_nms procedure (non-maximum supression); det: (pobj,pcls,ci,x1,y1,x2,y2)\n",
    "def basic_nms(dets, iou_thresh = 0.5):\n",
    "    filtered_dets = []\n",
    "    dets.sort(reverse=True)                     # lexicographically sort dets\n",
    "    while len(dets) > 0:                        # any remaining dets to check?\n",
    "        c = dets[0]                             # get current det\n",
    "        filtered_dets.append(c)                 # add to filtered_dets\n",
    "        # predicate remove dets with same class index and high iou\n",
    "        pred = lambda d : not (c[2] == d[2] and iou(c[3:], d[3:]) > iou_thresh)\n",
    "        dets = [d for d in dets if pred(d)]     # make list of remaining dets\n",
    "    return filtered_dets\n",
    "\n",
    "# make_annos procedure\n",
    "def make_annos(dets):\n",
    "    annos = []\n",
    "    for det in dets:\n",
    "        box = [det[4], det[3], det[6], det[5]]  # NOTE: replicate ONNX bug\n",
    "        score = det[0] * det[1]\n",
    "        cls = det[2]\n",
    "        annos.append((box, score, cls))\n",
    "    return annos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h1, w1, c1) = dog.shape\n",
    "(letterboxed_dog, (dw, dh), (w0, h0)) = letterbox(dog, (416, 416, 3))\n",
    "dog_dets1 = proc_results(dog_results1, coco_names,0.04, 0.04)\n",
    "#filtered_dets = basic_nms(dog_dets1, 0.5)\n",
    "dog_annos1 = make_annos(dog_dets1)\n",
    "\n",
    "# # unscale annotations to draw in original image frame\n",
    "dog_unscaled1 = unscale_annos(dog_annos1, dw, dh, w0, h0, w1, h1)\n",
    "\n",
    "# # draw list of annotations on original image\n",
    "dog_annotated1 = draw_annos(dog_raw, dog_unscaled1, coco_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dog_annotated1[..., ::-1]  )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(dog_unscaled1)} detections!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_wrapper(img, labels, pobj_thresh=0, pcls_thresh=0,iou_thresh=0,\n",
    "                      NMS=False):\n",
    "    (h1, w1, c1) = img.shape\n",
    "    (letterboxed_img, (dw, dh), (w0,h0)) = letterbox(img, (416, 416, 3))\n",
    "    res1 = run_inference(model, letterboxed_img)\n",
    "    dets1 = proc_results(res1, labels, pobj_thresh,pcls_thresh)\n",
    "    if NMS:\n",
    "        print(\"Applying NMS\")\n",
    "        filtered_dets1 = basic_nms(dets1, iou_thresh)\n",
    "        annos1 = make_annos(filtered_dets1)\n",
    "    else:\n",
    "        annos1 = make_annos(dets1)\n",
    "\n",
    "    unscaled_annos1 = unscale_annos(annos1, dw, dh, w0, h0, w1, h1)\n",
    "    print(f\"There are {len(unscaled_annos1)} detections!\")\n",
    "    annotated_img = draw_annos(img, unscaled_annos1, labels)\n",
    "\n",
    "    return annotated_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_aerial = inference_wrapper(aerial, coco_names, .05, .05)\n",
    "plt.imshow(annotated_aerial)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Post-processing YOLO Results with Non-Max Suppression (NMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could have done even worse\n",
    "dets = proc_results(dog_results1, coco_names, pobj_thresh=0, pcls_thresh=0)\n",
    "print(f\"When we set the thresholds to 0, there are {len(dets)} detections!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why?\n",
    "# (13 * 13 cells) * (3 anchor boxes * 80 classes)\n",
    "(13 * 13 * 3 * 80) + (26 * 26 * 3 * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better on images by playing around with the various post-processing thresholds we control?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set new thresholds and add NMS to try to reduce the number of detections for the dog\n",
    "\n",
    "OBJ_THRESH = 0.05\n",
    "CLASS_THRESH = 0.05\n",
    "IOU_THRESH = 0.9\n",
    "\n",
    "better_dog = inference_wrapper(dog, coco_names, pobj_thresh=OBJ_THRESH,\n",
    "                               pcls_thresh=CLASS_THRESH, iou_thresh=IOU_THRESH, NMS=True)\n",
    "plt.imshow(better_dog)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the aerial image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the aerial image?\n",
    "\n",
    "OBJ_THRESH = 0.01\n",
    "CLASS_THRESH = 0.01\n",
    "IOU_THRESH = 0.7\n",
    "\n",
    "better_aerial = inference_wrapper(aerial, coco_names, pobj_thresh=OBJ_THRESH,\n",
    "                               pcls_thresh=CLASS_THRESH, iou_thresh=IOU_THRESH, NMS=True)\n",
    "plt.imshow(better_aerial)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try doing detection on other images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in a new image first\n",
    "# new image\n",
    "kite = arr1 = cv2.imread('../data/kite.jpg')\n",
    "kite = kite[..., ::-1] # BGR --> RGB\n",
    "plt.imshow(kite)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJ_THRESH = 0.01\n",
    "CLASS_THRESH = 0.01\n",
    "IOU_THRESH = 0.7\n",
    "\n",
    "annotated_kite = inference_wrapper(kite, coco_names, pobj_thresh=OBJ_THRESH,\n",
    "                               pcls_thresh=CLASS_THRESH, iou_thresh=IOU_THRESH, NMS=True)\n",
    "plt.imshow(annotated_kite)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The `data` subdirectory contains the following images:  \n",
    "StopSign_Blue.jpg       horses.jpg              rareplanes4.jpg\n",
    "StopSign_Green.jpg      kite.jpg                rareplanes5.jpg\n",
    "StopSign_Red.jpg        rareplanes1.jpg         rareplanes6.jpg\n",
    "StopSign_Yellow.jpg     rareplanes10.jpg        rareplanes7.jpg\n",
    "dog.jpg                 rareplanes2.jpg         rareplanes8.jpg\n",
    "eagle.jpg               rareplanes3.jpg         rareplanes9.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try loading in and doing inference on these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
